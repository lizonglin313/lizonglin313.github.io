<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>爬取猫眼Top100 | Gridea</title>
<link rel="shortcut icon" href="https://lizonglin313.github.io//favicon.ico?v=1579616313288">
<link href="https://cdn.remixicon.com/releases/v2.1.0/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://lizonglin313.github.io//styles/main.css">
<link rel="alternate" type="application/atom+xml" title="爬取猫眼Top100 | Gridea - Atom Feed" href="https://lizonglin313.github.io//atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



    <meta name="description" content="
这是教材上的一个实例，今天学的总结一下。
首先感谢一下同老师送滴书哼
顺便推一下这本书：《Python3网路爬虫开发实战》（崔庆才）

先给代码

Pycharm + py3.7 调过
不很长50行左右，爬一下猫眼top100的一些信息
..." />
    <meta name="keywords" content="爬虫入门,Python" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://lizonglin313.github.io/">
  <img class="avatar" src="https://lizonglin313.github.io//images/avatar.png?v=1579616313288" alt="">
  </a>
  <h1 class="site-title">
    Gridea
  </h1>
  <p class="site-description">
    温故而知新
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          归档
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              爬取猫眼Top100
            </h2>
            <div class="post-info">
              <span>
                2019-03-20
              </span>
              <span>
                6 min read
              </span>
              
                <a href="https://lizonglin313.github.io/tag/Ep2MIuv-4" class="post-tag">
                  # 爬虫入门
                </a>
              
                <a href="https://lizonglin313.github.io/tag/ywt-yiKCrM" class="post-tag">
                  # Python
                </a>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content">
                <blockquote>
<p>这是教材上的一个实例，今天学的总结一下。</p>
<p>首先感谢一下同老师送滴书哼</p>
<p>顺便推一下这本书：《Python3网路爬虫开发实战》（崔庆才）</p>
</blockquote>
<h2 id="先给代码">先给代码</h2>
<blockquote>
<p>Pycharm + py3.7 调过</p>
<p>不很长50行左右，爬一下猫眼top100的一些信息</p>
</blockquote>
<pre><code class="language-python">from builtins import len, open, str, range
import json
import time
from requests.exceptions import RequestException
import requests
import re

def get_one_page(url):
    try:
        # 伪装请求头改为Chrome
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) '
                          'Chome/65.0.3325.162 Safari/537.36'
        }

        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            return response.text
        return None
    except RequestException:
        return None


def parse_one_page(html):
    pattern = re.compile(
        '&lt;dd&gt;.*?board-index.*?&gt;(.*?)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;.*?name.*?a.*?&gt;(.*?)&lt;/a&gt;.*?star.*?&gt;(.*?)&lt;/p&gt;.*?releasetime.*?&gt;(.*?)&lt;/p&gt;' 
        '.*?integer.*?&gt;(.*?)&lt;/i&gt;.*?fraction.*?&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;', re.S
    )
    items = re.findall(pattern, html)
    for item in items:
        yield {
            'index': item[0],
            'image': item[1],
            'title': item[2],
            'actor': item[3].strip()[3:],
            'time': item[4].strip()[5:],
            'score': item[5] + item[6]
        }

def write_to_file(content):
    with open('result_maoyan.txt', 'a', encoding='utf-8') as f:
        f.write(json.dumps(content, ensure_ascii=False) + '\n')


def main(offset):
    url = 'https://maoyan.com/board/4?offset=' + str(offset)
    html = get_one_page(url)
    for item in parse_one_page(html):
        print (item)
        write_to_file(item)


if __name__ == '__main__':
    for i in range(10):
        main(offset = i*10)
        time.sleep(1)
</code></pre>
<h2 id="详解">详解</h2>
<h3 id="首先是抓取单页请求信息一些内容直接码在代码里了">首先是抓取单页请求信息，一些内容直接码在代码里了</h3>
<pre><code class="language-python">def get_one_page(url):
    try:
        # 伪装请求头改为Chrome， 配置头信息
        # 猫眼有反爬，如果被识别为爬虫会给你返回个大嘴巴子，这里伪装Chrome浏览器
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) '
                          'Chome/65.0.3325.162 Safari/537.36'
        }
		# 使用requests方法
        response = requests.get(url, headers=headers)
        # 通过状态码判断是否出现问题
        if response.status_code == 200:
            # 如果没有问题就返回抓取的页面信息
            return response.text
        return None
    except RequestException:		 # 我想良好的代码风格应该少不了try-expect
        return None
</code></pre>
<h3 id="第二部分主要是通过正则表达式提取有价值信息主要就是正则表达式的使用">第二部分主要是通过正则表达式提取有价值信息，主要就是正则表达式的使用</h3>
<blockquote>
<p>有部分涉及到 yield 的使用，贴一篇<a href="https://blog.csdn.net/mieleizhi0522/article/details/82142856">前辈的文章</a>，关于 yield 讲的比较清楚</p>
</blockquote>
<pre><code class="language-python">def parse_one_page(html):
    pattern = re.compile(
        '&lt;dd&gt;.*?board-index.*?&gt;(.*?)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;.*?name.*?a.*?&gt;(.*?)&lt;/a&gt;.*?star.*?&gt;(.*?)&lt;/p&gt;.*?releasetime.*?&gt;(.*?)&lt;/p&gt;' 
        '.*?integer.*?&gt;(.*?)&lt;/i&gt;.*?fraction.*?&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;', re.S
    )
    # re.compile()是把字符串转化成正则表达式常用的方法，之后使用findall方法
    items = re.findall(pattern, html)
    for item in items:
        # yield 是我第一次用，就类似于特殊的return，返回设定的字典数据类型
        yield {
            'index': item[0],
            'image': item[1],
            'title': item[2],
            'actor': item[3].strip()[3:],
            'time': item[4].strip()[5:],
            'score': item[5] + item[6]
        }

</code></pre>
<p>关于正则表达式，我也是好久才明白，其实匹配的就是 (.*?) 里面的东西。我们看下面的正则表达式：</p>
<pre><code class="language-python">    pattern = re.compile(
        '&lt;dd&gt;.*?board-index.*?&gt;(.*?)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;.*?name.*?a.*?&gt;(.*?)&lt;/a&gt;.*?star.*?&gt;(.*?)&lt;/p&gt;.*?releasetime.*?&gt;(.*?)&lt;/p&gt;' 
        '.*?integer.*?&gt;(.*?)&lt;/i&gt;.*?fraction.*?&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;', re.S
    )
</code></pre>
<p>现来分析一下网页源码：</p>
<figure data-type="image" tabindex="1"><img src="https://i.loli.net/2019/03/21/5c93101343030.jpg" alt="" loading="lazy"></figure>
<p>我们主要提取以下内容：</p>
<ul>
<li>排名信息</li>
<li>电影图片</li>
<li>电影名称</li>
<li>主演</li>
<li>发布时间</li>
<li>评分</li>
</ul>
<p>就以排名信息为栗子：</p>
<figure data-type="image" tabindex="2"><img src="https://i.loli.net/2019/03/21/5c93111b0ab9f.jpg" alt="" loading="lazy"></figure>
<p>我们所要获取的，就是 1 这个数字，正则表达式如下：</p>
<pre><code class="language-python">&lt;dd&gt;.*?board-index.*?&gt;(.*?)&lt;/i&gt;
</code></pre>
<p>这一部分开始于 &lt; dd &gt; 结束于 &lt; /i &gt; 后面的那个 ( .*? ) 就是我们要匹配的内容。</p>
<h3 id="然后我们写入文件">然后我们写入文件</h3>
<pre><code class="language-python">def write_to_file(content):
    with open('result_maoyan.txt', 'a', encoding='utf-8') as f:
        f.write(json.dumps(content, ensure_ascii=False) + '\n')
</code></pre>
<h3 id="调用函数是这样的">调用函数是这样的</h3>
<pre><code class="language-python">def main(offset):
    url = 'https://maoyan.com/board/4?offset=' + str(offset)
    html = get_one_page(url)
    for item in parse_one_page(html):
        print (item)
        write_to_file(item)
</code></pre>
<p>由于猫眼每页只显示10条信息，我们先看一下URL有什么规律：</p>
<p>第一页：</p>
<figure data-type="image" tabindex="3"><img src="https://i.loli.net/2019/03/21/5c93144ca35a6.jpg" alt="" loading="lazy"></figure>
<p>第二页：</p>
<figure data-type="image" tabindex="4"><img src="https://i.loli.net/2019/03/21/5c931465c9cf2.jpg" alt="" loading="lazy"></figure>
<p>第三页：</p>
<figure data-type="image" tabindex="5"><img src="https://i.loli.net/2019/03/21/5c9314852ed9d.jpg" alt="" loading="lazy"></figure>
<p>懂了吧，offset就是传入的那个最后的数据，之后调用请求获取当页信息的函数，写入文件的函数，就OK了。</p>
<h3 id="主函数">主函数</h3>
<pre><code class="language-python">if __name__ == '__main__':
    for i in range(10):
        main(offset = i*10)
        time.sleep(1)
</code></pre>
<p>这里用了 time.sleep 方法，如果我们访问时间过快，会被判定为非正常访问就会被封 IP 不能再继续爬取，这里设定间隔 1 s。</p>
<h2 id="总结">总结</h2>
<p>几个关键点：</p>
<ol>
<li>
<p>请求头处理。</p>
<p>一般来讲，现在许多网站都有反爬措施，如果检测请求头，发现是来自爬虫，那么多半会拒绝你的访问，所以这里修改了请求头。</p>
</li>
<li>
<p>正则表达式。</p>
<p>这个还是要多练多去看别人是怎么写的。</p>
</li>
<li>
<p>请求时间。</p>
<p>不要过快就好，模仿人类速度。</p>
</li>
</ol>
<blockquote>
<p>要开始学Go了，后面的项目有涉及到区块链，有时间更一篇区块链的吧。还有非自然死亡真的太好看了8也！！！</p>
</blockquote>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#%E5%85%88%E7%BB%99%E4%BB%A3%E7%A0%81">先给代码</a></li>
<li><a href="#%E8%AF%A6%E8%A7%A3">详解</a>
<ul>
<li><a href="#%E9%A6%96%E5%85%88%E6%98%AF%E6%8A%93%E5%8F%96%E5%8D%95%E9%A1%B5%E8%AF%B7%E6%B1%82%E4%BF%A1%E6%81%AF%E4%B8%80%E4%BA%9B%E5%86%85%E5%AE%B9%E7%9B%B4%E6%8E%A5%E7%A0%81%E5%9C%A8%E4%BB%A3%E7%A0%81%E9%87%8C%E4%BA%86">首先是抓取单页请求信息，一些内容直接码在代码里了</a></li>
<li><a href="#%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%E4%B8%BB%E8%A6%81%E6%98%AF%E9%80%9A%E8%BF%87%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%8F%90%E5%8F%96%E6%9C%89%E4%BB%B7%E5%80%BC%E4%BF%A1%E6%81%AF%E4%B8%BB%E8%A6%81%E5%B0%B1%E6%98%AF%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E7%9A%84%E4%BD%BF%E7%94%A8">第二部分主要是通过正则表达式提取有价值信息，主要就是正则表达式的使用</a></li>
<li><a href="#%E7%84%B6%E5%90%8E%E6%88%91%E4%BB%AC%E5%86%99%E5%85%A5%E6%96%87%E4%BB%B6">然后我们写入文件</a></li>
<li><a href="#%E8%B0%83%E7%94%A8%E5%87%BD%E6%95%B0%E6%98%AF%E8%BF%99%E6%A0%B7%E7%9A%84">调用函数是这样的</a></li>
<li><a href="#%E4%B8%BB%E5%87%BD%E6%95%B0">主函数</a></li>
</ul>
</li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://lizonglin313.github.io/post/Python伪造数据生成器：Faker">
              <h3 class="post-title">
                Python伪造数据生成器：Faker
              </h3>
            </a>
          </div>
        

        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
  <a class="rss" href="https://lizonglin313.github.io//atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
